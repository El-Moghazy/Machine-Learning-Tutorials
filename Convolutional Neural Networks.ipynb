{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "axI4u1EkzGmT",
        "colab_type": "text"
      },
      "source": [
        "#Convolutional Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##In this tutorial I will be using Keras with TensorFlow as backend to calssify digits from the MNIST Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "Dd3EpSgL1Kr7",
        "colab_type": "text"
      },
      "source": [
        "First you need to install Keras using the following lines"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "HV9b4yun0vPq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8743424d-d76d-4545-f83c-38b484ce6998",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1516128111004,
          "user_tz": -120,
          "elapsed": 5304,
          "user": {
            "displayName": "Abd El Rhman ElMoghazy",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112798608963931725749"
          }
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3icmSjgt1VLP",
        "colab_type": "text"
      },
      "source": [
        "Then import Keras and all the layers and libraries we need"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "96_Dh7GYzFwv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "import numpy\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5tzF9kr10GX",
        "colab_type": "text"
      },
      "source": [
        "We then import the layers of the convolutional neural network.The network consists of two main components :\n",
        "\n",
        "1. Convolutional layers : the convolutional layer is responsible for the convolutional operation in which feature maps identifies features in the images.\n",
        "and is usually followed by two types of layers which are :\n",
        ">*   **Dropout** : Dropout is a regulization technique where you turn off part of the network's layers randomally to increase regulization and hense decrease overfitting. We use when the training set accuracy is muuch higher than the test set accuracy.\n",
        ">*   **Max Pooling** : The maximum output in a rectangular neighbourhood. It is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting the group of pixels that are highly contributing to each feature in the feature maps in the layer.\n",
        "2. Dense layers : The dense layer is a fully connected layer that comes after the convolutional layers and they give us the output vector of the Network.\n",
        "\n",
        "As a convention in Convolutional Neural Network we decrease the dimensions of the layers as we go deeper and increase the number of feature maps to make it detect more features and decrease the number of computational cost.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/Untitled.png)\n",
        "\n",
        " "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "EfQa_UJZ5wsR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "from keras.layers import Dense, Dropout,Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlAMFOZO9feO",
        "colab_type": "text"
      },
      "source": [
        "Sequential layers are stacked such that every layer passes its output to the next layer without you specifying extra information so we import Sequential from models"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "T71utABW92J8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "model = Sequential()"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ja6xaTVOELGP",
        "colab_type": "text"
      },
      "source": [
        "We must specify which data format convention Keras will follow using the following line of code. Keras can accept the number of channels before other dimensions or after it but here we have to specify which convention we will use. We will use channels last which is Tensorflow's convention ."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "_zASxWJTEJGr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "numpy.random.seed(0)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBqW5EfhZ9q1",
        "colab_type": "text"
      },
      "source": [
        "We should call mnist.load_data() which contains the mnist Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  when we call mnist.load_data() it returns two tuples one for the training set containing the images and their corresponding lables and another one for the test set."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "hAEk70qRbUs8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "674d2997-9754-4989-e278-3a515fb465bd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1516128115363,
          "user_tz": -120,
          "elapsed": 1701,
          "user": {
            "displayName": "Abd El Rhman ElMoghazy",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112798608963931725749"
          }
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JDA0JlAMdIGs",
        "colab_type": "text"
      },
      "source": [
        "We then reshape the samples according to TensorFlow convention which we chosed previously using \"K.set_image_data_format('channels_last')\" samples,rows,columns,channels as we are using channels_last if you are using channels_first you will need to change the order to samples,channels,rows,column and here we have only one channel because we are using the image in grayscale not RGB."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "Ja_ksnoldSBI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28 , 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28 , 1).astype('float32')"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxffE-VjhUIs",
        "colab_type": "text"
      },
      "source": [
        "To increase the efficiency and the convergence of the algorithm we normalize the data based on the fact that the pixels' maximum value is 255 so we divide all the pixels by 255 to obtain results between 0 and 1."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "-uzfXJYohzj5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "X_train = X_train / 255\n",
        "X_test2 = X_test / 255"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nKwkWjk0iAba",
        "colab_type": "text"
      },
      "source": [
        "Making the output in the form of one vs all (aka one hot encoding) which means that we will have 10 calsses from 0 to 9 one class for each number from 0 to 9\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "_z58ybtgiA6I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dr-O3FdEjZMG",
        "colab_type": "text"
      },
      "source": [
        "Now lets implement the first layer of the convolutional network as shown in the schema below .\n",
        "![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/Layer.png)\n",
        "For the sequential model you just stack the layers and only specify the image input dimensions in the first layer.\n",
        "Our first layer will be a convolutional layer Conv2D() where we specify the number of feature maps , the input shape and the activation function which is here relu .The relu activation function is represented mathematically by max(0,X).\n",
        "We then add the max pooling layer (which is the most common kind of pooling) with a kernel of dimensions 2 * 2 .\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "SHZ0G1fykEPM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "model.add(Conv2D(30, (5, 5), input_shape=( 28, 28 , 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jnn9RQ92lh7L",
        "colab_type": "text"
      },
      "source": [
        "Lets add the 2nd layer but this time we increase the feature maps ."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "FCNLJgn5liic",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "model.add(Conv2D(70, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Zd7f3cel8A5",
        "colab_type": "text"
      },
      "source": [
        "Now we add a flatten layer that takes the output of the CNN and flattens it and passes it as an input to the Dense Layers which passes it to the output layer.\n",
        "we have used number of classes = 10 because we have 10 numbers from 0 to 9 .\n",
        "every dense layer contains 300 neurons except for the output layer.\n",
        "We use Softmax with the output layer to output estimated probability vector for  multi-class classification ."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "lieaM8XWl8kX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "num_classes = 10\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5wJGng0p6-B",
        "colab_type": "text"
      },
      "source": [
        "We have to compile the model and then try training it using the fit() function which fits the training data and labels , the number of epochs and the batch_size which is the number of photos per training cycle.\n",
        "The last thing that we are going to do is to evaluate the model to ensure that it doesn't overfit the trainig data .Evaluating the model is done by using the weights that resulted from the training step and using it to estimate the value of the test data that the model haven't seen before to estimate how well the model will perform in the future on new data.\n",
        "\n",
        "if you are using cross-validation split then the convention is to split the data by 60% training set , 20% validation set and 20% test set but in the era of big data this ratio may vary according to the amount of data you have."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "96_UW0g1hMDF",
        "colab_type": "text"
      },
      "source": [
        "We have used categorical_crossentropy as the cost function for that model but what does we mean by **cost function**\n",
        "\n",
        "####Cost function : It is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase.\n",
        "\n",
        "#### Optimizer : It is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms. You can refere to this paper to know how it works https://arxiv.org/abs/1412.6980v8\n",
        "\n",
        "You can use other metrics to measure the performance other than accuracy as precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and     F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "XhwUKUarp80w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 157
            },
            {
              "item_id": 286
            },
            {
              "item_id": 410
            },
            {
              "item_id": 534
            },
            {
              "item_id": 644
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2c577b4a-6d5f-4077-defb-ae45fb7a5716",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1516130986715,
          "user_tz": -120,
          "elapsed": 610614,
          "user": {
            "displayName": "Abd El Rhman ElMoghazy",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112798608963931725749"
          }
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs= 10, batch_size=200)\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose = 10)\n",
        "\n",
        "print ( scores )\n"
      ],
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 60s 999us/step - loss: 0.0073 - acc: 0.9974\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 3/10\n",
            " 3000/60000 [>.............................] - ETA: 57s - loss: 0.0025 - acc: 0.9993"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0060 - acc: 0.9981\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0032 - acc: 0.9989\n",
            "Epoch 5/10\n",
            "10000/60000 [====>.........................] - ETA: 50s - loss: 0.0023 - acc: 0.9993"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0030 - acc: 0.9991\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 7/10\n",
            "11400/60000 [====>.........................] - ETA: 49s - loss: 0.0087 - acc: 0.9973"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0037 - acc: 0.9987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0044 - acc: 0.9985\n",
            "Epoch 9/10\n",
            "12800/60000 [=====>........................] - ETA: 47s - loss: 0.0052 - acc: 0.9981"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0038 - acc: 0.9988\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 8.1473e-04 - acc: 0.9998\n",
            "[0.11124998168051242, 0.9931]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYNAQuZxtHIQ",
        "colab_type": "text"
      },
      "source": [
        "###This tutorial is written by AbdElRhman ElMoghazy.\n",
        "\n",
        "### Refrences ,Textbooks and Tutorials :\n",
        "Hands on machine learning with scikit-learn and TensorFlow by Aurélien Géron\n",
        "\n",
        "Pyhron machine learning 2nd edition by Sebastian Raschka ,Vahid Mirjalili\n",
        "\n",
        "http://www.deeplearningbook.org/\n",
        "\n",
        "https://keras.io/\n",
        "\n",
        "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
        "\n",
        "https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/index.html?index=..%2F..%2Findex#0"
      ],
      "cell_type": "markdown"
    }
  ]
}